{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a CNN audio classifier using melspectograms from Audioset and ESC-50 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16572324929097342438\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 56033280\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 17993344051382349124\n",
      "physical_device_desc: \"device: 0, name: Quadro P5000, pci bus id: 0000:00:05.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(file_paths, label_dir, train_split):\n",
    "    # get new train and validation indices\n",
    "    n_files = len(file_paths)\n",
    "    n_train = int(train_split * n_files)\n",
    "    train_idx = random.sample(range(n_files), n_train)\n",
    "    val_idx = list(set(range(n_files)) - set(train_idx))\n",
    "    \n",
    "    # split files\n",
    "    train_set = [file_paths[idx] for idx in train_idx]\n",
    "    val_set = [file_paths[idx] for idx in val_idx]\n",
    "\n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save train and validation splits into tmp directories\n",
    "def train_and_val_split(path_to_train_set, train_split, seed=23):\n",
    "    random.seed(seed)\n",
    "\n",
    "    train_labels = os.listdir(path_to_train_set)\n",
    "    print '{} classes observed'.format(len(train_labels))\n",
    "    \n",
    "    # split original files\n",
    "    train_dir = '/tmp/train'\n",
    "    val_dir = '/tmp/val'\n",
    "    n_train_files = 0\n",
    "    n_val_files = 0\n",
    "    for label_dir in train_labels:\n",
    "        # fill in dictionary\n",
    "        full_path_to_dir = os.path.join(path_to_train_set, label_dir)\n",
    "        files = os.listdir(full_path_to_dir) \n",
    "        file_paths = [os.path.join(path_to_train_set, label_dir, f) for f in files]\n",
    "        train_set, val_set = split_dataset(file_paths, label_dir, train_split)\n",
    "        \n",
    "        # copy files over to tmp directories\n",
    "        train_label_dir = os.path.join(train_dir, label_dir)\n",
    "        if os.path.exists(train_label_dir):\n",
    "            shutil.rmtree(train_label_dir)\n",
    "        os.makedirs(train_label_dir)\n",
    "            \n",
    "        val_label_dir = os.path.join(val_dir, label_dir)\n",
    "        if os.path.exists(val_label_dir):\n",
    "            shutil.rmtree(val_label_dir)\n",
    "        os.makedirs(val_label_dir)\n",
    "        \n",
    "        def tmp_train_path(f):\n",
    "            base_filename = os.path.basename(f)\n",
    "            return os.path.join(train_label_dir, base_filename)\n",
    "        def tmp_val_path(f):\n",
    "            base_filename = os.path.basename(f)\n",
    "            return os.path.join(val_label_dir, base_filename)\n",
    "        \n",
    "        map(lambda f: shutil.copyfile(f, tmp_train_path(f)), train_set)\n",
    "        map(lambda f: shutil.copyfile(f, tmp_val_path(f)), val_set)\n",
    "        \n",
    "        n_train_files += len(train_set)\n",
    "        n_val_files += len(val_set)\n",
    "                 \n",
    "    print 'Found {} files for train set'.format(n_train_files)\n",
    "    print 'Found {} files for validaiton set'.format(n_val_files)\n",
    "                 \n",
    "    return train_dir, val_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = 'continuous_wavelet_transform'\n",
    "batch_size = 40\n",
    "epochs = 150\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "input_tensor = Input(shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure training and validation data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_dir = '../hackathon_dataset/{}/train'.format(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3716 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# training generator configuration\n",
    "training_datagen = image.ImageDataGenerator(\n",
    "    rescale=1./255)\n",
    "\n",
    "training_generator = training_datagen.flow_from_directory(\n",
    "    training_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_dir = '../hackathon_dataset/{}/validation'.format(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1219 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# validation generator configuration\n",
    "validation_datagen = image.ImageDataGenerator(\n",
    "    rescale=1./255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_training_samples = 3716\n",
    "nb_validation_samples = 1219\n",
    "n_classes = len(os.listdir(training_data_dir)) - 1\n",
    "class_labels = os.listdir(training_data_dir)[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[3,3,512,512]\n\t [[Node: block4_conv3_1/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=8090260, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](block4_conv3_1/random_uniform/shape)]]\n\nCaused by op u'block4_conv3_1/random_uniform/RandomUniform', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-428d6a3d251c>\", line 2, in <module>\n    pretrained_model = VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/applications/vgg16.py\", line 129, in VGG16\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 569, in __call__\n    self.build(input_shapes[0])\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py\", line 134, in build\n    constraint=self.kernel_constraint)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py\", line 88, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 391, in add_weight\n    weight = K.variable(initializer(shape), dtype=dtype, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/initializers.py\", line 208, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 3446, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/random_ops.py\", line 236, in random_uniform\n    shape, dtype, seed=seed1, seed2=seed2)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_random_ops.py\", line 249, in _random_uniform\n    seed=seed, seed2=seed2, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[3,3,512,512]\n\t [[Node: block4_conv3_1/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=8090260, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](block4_conv3_1/random_uniform/shape)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-428d6a3d251c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'vgg16'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpretrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model loaded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/applications/vgg16.pyc\u001b[0m in \u001b[0;36mVGG16\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[1;32m    167\u001b[0m                                     \u001b[0mWEIGHTS_PATH_NO_TOP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                                     cache_subdir='models')\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'theano'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mlayer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_all_kernels_in_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   2570\u001b[0m             \u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2571\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2572\u001b[0;31m             \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m   3006\u001b[0m                              ' elements.')\n\u001b[1;32m   3007\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3008\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2187\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2189\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muninitialized_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3,3,512,512]\n\t [[Node: block4_conv3_1/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=8090260, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](block4_conv3_1/random_uniform/shape)]]\n\nCaused by op u'block4_conv3_1/random_uniform/RandomUniform', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-428d6a3d251c>\", line 2, in <module>\n    pretrained_model = VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/applications/vgg16.py\", line 129, in VGG16\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 569, in __call__\n    self.build(input_shapes[0])\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py\", line 134, in build\n    constraint=self.kernel_constraint)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py\", line 88, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 391, in add_weight\n    weight = K.variable(initializer(shape), dtype=dtype, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/initializers.py\", line 208, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 3446, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/random_ops.py\", line 236, in random_uniform\n    shape, dtype, seed=seed1, seed2=seed2)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_random_ops.py\", line 249, in _random_uniform\n    seed=seed, seed2=seed2, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[3,3,512,512]\n\t [[Node: block4_conv3_1/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=8090260, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](block4_conv3_1/random_uniform/shape)]]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'vgg16'\n",
    "pretrained_model = VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "print('Model loaded')\n",
    "print pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'vgg16 contain {} layers'.format(len(pretrained_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers_to_freeze = len(pretrained_model.layers) - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               3211392   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 3,211,650\n",
      "Trainable params: 3,211,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=pretrained_model.output_shape[1:]))\n",
    "top_model.add(Dense(128, activation='relu'))\n",
    "top_model.add(Dropout(0.6))\n",
    "top_model.add(Dense(n_classes, activation='softmax'))\n",
    "top_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine base model with top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 2)                 3211650   \n",
      "=================================================================\n",
      "Total params: 17,926,338\n",
      "Trainable params: 17,926,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# top_model.load_weights('bootlneck_fc_model.h5')\n",
    "model = Model(inputs=pretrained_model.input, outputs=top_model(pretrained_model.output))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('models/json_models'):\n",
    "    os.makedirs('models/json_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics, optimizers\n",
    "\n",
    "def top_5_accuracy(y_true, y_pred):\n",
    "    return metrics.top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
    "\n",
    "for layer in model.layers[:num_layers_to_freeze]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# regualr SGD\n",
    "model.compile(optimizer=optimizers.SGD(lr=1e-4, momentum=0.9, decay=1e-6, nesterov=True), \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "model_filename = \"models/json_models/audioset_nesterov_{}_{}_{}_frozen_layers_dropout_60pct.json\"\\\n",
    "                    .format(model_name, data_source, num_layers_to_freeze)\n",
    "with open(model_filename, \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('models/weights'):\n",
    "    os.makedirs('models/weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.6557 - acc: 0.6308Epoch 00000: val_acc improved from -inf to 0.67917, saving model to vgg16_melspec_weights_freeze_17_base_layers.best.hdf5\n",
      "92/92 [==============================] - 56s - loss: 0.6556 - acc: 0.6307 - val_loss: 0.5745 - val_acc: 0.6792\n",
      "Epoch 2/10\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5920 - acc: 0.6809Epoch 00001: val_acc improved from 0.67917 to 0.72010, saving model to vgg16_melspec_weights_freeze_17_base_layers.best.hdf5\n",
      "92/92 [==============================] - 46s - loss: 0.5913 - acc: 0.6822 - val_loss: 0.5366 - val_acc: 0.7201\n",
      "Epoch 3/10\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5588 - acc: 0.7035Epoch 00002: val_acc improved from 0.72010 to 0.74300, saving model to vgg16_melspec_weights_freeze_17_base_layers.best.hdf5\n",
      "92/92 [==============================] - 45s - loss: 0.5592 - acc: 0.7021 - val_loss: 0.5171 - val_acc: 0.7430\n",
      "Epoch 4/10\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5488 - acc: 0.7134Epoch 00003: val_acc improved from 0.74300 to 0.75997, saving model to vgg16_melspec_weights_freeze_17_base_layers.best.hdf5\n",
      "92/92 [==============================] - 45s - loss: 0.5481 - acc: 0.7133 - val_loss: 0.4955 - val_acc: 0.7600\n",
      "Epoch 5/10\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5254 - acc: 0.7323Epoch 00004: val_acc improved from 0.75997 to 0.76845, saving model to vgg16_melspec_weights_freeze_17_base_layers.best.hdf5\n",
      "92/92 [==============================] - 45s - loss: 0.5246 - acc: 0.7331 - val_loss: 0.4906 - val_acc: 0.7684\n",
      "Epoch 6/10\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5310 - acc: 0.7205Epoch 00005: val_acc improved from 0.76845 to 0.77184, saving model to vgg16_melspec_weights_freeze_17_base_layers.best.hdf5\n",
      "92/92 [==============================] - 45s - loss: 0.5311 - acc: 0.7197 - val_loss: 0.4845 - val_acc: 0.7718\n",
      "Epoch 7/10\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5140 - acc: 0.7391Epoch 00006: val_acc did not improve\n",
      "92/92 [==============================] - 44s - loss: 0.5152 - acc: 0.7378 - val_loss: 0.4954 - val_acc: 0.7430\n",
      "Epoch 8/10\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5111 - acc: 0.7331Epoch 00007: val_acc did not improve\n",
      "92/92 [==============================] - 44s - loss: 0.5116 - acc: 0.7327 - val_loss: 0.4939 - val_acc: 0.7566\n",
      "Epoch 9/10\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5016 - acc: 0.7418Epoch 00008: val_acc did not improve\n",
      "92/92 [==============================] - 44s - loss: 0.5017 - acc: 0.7408 - val_loss: 0.4794 - val_acc: 0.7557\n",
      "Epoch 10/10\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5033 - acc: 0.7455Epoch 00009: val_acc improved from 0.77184 to 0.79729, saving model to vgg16_melspec_weights_freeze_17_base_layers.best.hdf5\n",
      "92/92 [==============================] - 45s - loss: 0.5027 - acc: 0.7458 - val_loss: 0.4460 - val_acc: 0.7973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1b1831c710>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from time import time\n",
    "\n",
    "# set up log files for tensorboard\n",
    "tensorboard = TensorBoard(log_dir=\"logs/vgg16_{}_layers_frozen\".format(num_layers_to_freeze))\n",
    "\n",
    "# configure early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"vgg16_melspec_weights_freeze_{}_base_layers.best.hdf5\".format(num_layers_to_freeze)\n",
    "best_model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [best_model_checkpoint, tensorboard, early_stopping]\n",
    "\n",
    "model.fit_generator(\n",
    "    training_generator,\n",
    "    steps_per_epoch=nb_training_samples/batch_size,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples/batch_size,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate and plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWkAAAFfCAYAAACMWD3+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyzAnaTOzDHOSNjPLsP8HKxHxh07wY1AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ab259a250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for label in class_labels:\n",
    "    file_list = os.listdir(validation_data_dir + '/' + label)\n",
    "    for file_name in file_list:\n",
    "        img_path = validation_data_dir + '/' + label + '/' + file_name\n",
    "        \n",
    "        img = image.load_img(img_path, target_size=(224, 224))\n",
    "        \n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)* 1./255\n",
    "        \n",
    "        preds = model.predict(x)[0]\n",
    "        \n",
    "        y_true.append(label)\n",
    "        y_pred.append(class_labels[np.argmax(preds)])\n",
    "        \n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cm, sorted(class_labels), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
